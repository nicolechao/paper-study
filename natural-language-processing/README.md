# Natural Language Processing


## Survey Paper
1. Comparative Study of CNN and RNN for Natural Language Processing, Wenpeng Yin, Katharina Kann, Mo Yu and Hinrich Schutze. 2017. [Paper](https://arxiv.org/pdf/1702.01923.pdf).

## Journal and Conference Paper
1. Long Short-Term Memory, Sepp Hochreiter and Jurgen Schmidhuber, 1997. [Paper](https://www.bioinf.jku.at/publications/older/2604.pdf).
2. mRNN: Generating Text with Recurrent Neural Networks, Ilya Sutskever, James Martens, Geoffrey Hinton, 2011. [Paper](https://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf).
3. word2vec: Distributed Representations of Words and Phrases
and their Compositionality, Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean. 2013. [Paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).
4. Distributed Representations of Sentences and Documents, Quoc V. Le, Tomas Mikolov, 2014. [Paper](https://arxiv.org/pdf/1405.4053.pdf).
5. VADER: A Parsimonious Rule-based Model for
Sentiment Analysis of Social Media Text, C.J. Hutto, Eric Gilbert. 2014. [Paper](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf).
6. Convolutional Neural Networks for Sentence Classification, Yoon Kim. 2014. [Paper](https://www.aclweb.org/anthology/D14-1181).
7. Character-level Convolutional Networks for Text
Classification, Xiang Zhang, Junbo Zhao, Yann LeCun. 2015. [Paper](https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf)
8. mLSTM: Multiplicative LSTM for sequence modelling, Ben Krause, Liang Lu, Iain Murray, Steve Renals. 2017. [Paper](https://arxiv.org/pdf/1609.07959.pdf). 
9. Learning to Generate Reviews and Discovering Sentiment, Alec Radford, Rafal Jozefowicz, Ilya Sutskever. 2017. [Paper](https://arxiv.org/pdf/1704.01444.pdf), [Code](https://github.com/titu1994/Keras-Multiplicative-LSTM).
10. TCN: An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling, Shaojie Bai, J. Zico Kolter, Vladlen Koltun. 2018. [Paper](https://arxiv.org/pdf/1803.01271.pdf).
11. BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding, Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. [Paper](https://arxiv.org/pdf/1810.04805.pdf).